{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyas-muralidhara/Enron-Email-Priority-Sorting-Response-Recommendation/blob/main/Email_Page_rank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvzzu1qxfsYn"
      },
      "source": [
        "## Page Ranking for Enron Email Dataset\n",
        "\n",
        "CSC791 - Natural Language Processing Spring 2020\n",
        "\n",
        "Author - Shreyas Chikkballapur Muralidhara - schikkb  \n",
        "          Sharath Narayana - snaraya9\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT85aH2v4vep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f653e7a-864b-4e3f-b79f-feec07032696"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRC2A8cgVyO"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0FraLjor7DK"
      },
      "source": [
        "!tar -xvf '/content/drive/Shared drives/NLP Project/enron_mail_20150507.tar.gz' -C '/content/drive/Shared drives/NLP Project/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJzgy9m2mg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4729fa0-278f-4d69-9a0a-1eee46a86a0c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xlrd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score,accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC,LinearSVC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAEbckIGaolM"
      },
      "source": [
        "Load the dataset into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyW3HwLmP3KI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c623c962-a05e-412c-ff51-7f8823f35368"
      },
      "source": [
        "email_df = pd.read_csv('/content/drive/Shared drives/NLP Project/test.csv',index_col=None,header=0)\n",
        "email_df.Date_list = email_df.Date_list.str.replace(',','')\n",
        "print(email_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9571, 7)\n",
            "[\"kmann (Non-Privileged).pst\\n\\n15:37:55 Synchronizing Mailbox 'Mann, Kay'\\n15:37:55 Synchronizing Hierarchy\\n15:37:56 Synchronizing Favorites\\n15:37:56 Synchronizing Folder 'Inbox'\\n15:38:12 \"]\n",
            "ynchronization Log:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od8OMBMya3Yu"
      },
      "source": [
        "Extracting Profile Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEgWJEc53yqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077c92a4-31cb-44fa-d683-eadda1dda97c"
      },
      "source": [
        "# Consider the necessary columns, without the category & labels\n",
        "features_df = email_df\n",
        "\n",
        "\n",
        "\n",
        "# Select the top  receivers of emails, extract the emails sent by them - \n",
        "# 1.SYMES-K(kate.symes@enron.com)\n",
        "# 2.DASOVICH-J(jeff.dasovich@enron.com)\n",
        "# 3.MANN-K(kay.mann@enron.com)\n",
        "\n",
        "#Extracting Profile 1\n",
        "#extract the emails that were sent to SYMES-K and received by SYMES-K\n",
        "# SYMESK_df = features_df[features_df['To'].str.contains('kate.symes@enron.com', na=False)]\n",
        "# SYMESK_df = SYMESK_df.append(features_df[features_df['From'].str.contains('kate.symes@enron.com', na=False)], ignore_index=True)\n",
        "# print(SYMESK_df.shape)\n",
        "\n",
        "# #Extracting Profile 2\n",
        "# #extract the emails that were sent to DASOVICH-J and received by DASOVICH-J\n",
        "# DASOVICHJ_df = features_df[features_df['To'].str.contains('jeff.dasovich@enron.com', na=False)]\n",
        "# DASOVICHJ_df = DASOVICHJ_df.append(features_df[features_df['From'].str.contains('jeff.dasovich@enron.com', na=False)], ignore_index=True)\n",
        "# print(DASOVICHJ_df.shape)\n",
        "\n",
        "#Extracting Profile 3\n",
        "#extract the emails that were sent to MANN-K and received by MANN-K\n",
        "MANNK_df = features_df[features_df['To_list'].str.contains('kay.mann@enron.com', na=False)]\n",
        "MANNK_df = MANNK_df.append(features_df[features_df['From_list'].str.contains('kay.mann@enron.com', na=False)], ignore_index=True)\n",
        "print(MANNK_df.shape)\n",
        "\n",
        "#features_df.groupby(['Subject'])['Subject'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(50)# Cat_1_level_2.unique()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8647, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWPgXWhrxxMd"
      },
      "source": [
        "Page Ranking \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFkh2aZNjPrp"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "class PageRank:\n",
        "\n",
        "\tdef __init__(self, beta, edges, epsilon, max_iterations, node_num, weight_vector):\n",
        "\t\tself.beta = beta\n",
        "\t\tself.edges = edges\n",
        "\t\tself.epsilon = epsilon\n",
        "\t\tself.node_num = node_num\n",
        "\t\tself.MAX_ITERATIONS = max_iterations\n",
        "\t\tself.weight_vector = weight_vector\n",
        "\n",
        "\tdef pageRank(self):\n",
        "\t\tfinal_rank_vector = np.zeros(self.node_num)\n",
        "\t\tinitial_rank_vector = np.fromiter(\n",
        "\t\t\t[1 / self.node_num for _ in range(self.node_num)], dtype='float')\n",
        "\t\titerations = 0\n",
        "\t\tdiff = math.inf\n",
        "\t\twhile(iterations < self.MAX_ITERATIONS and diff > self.epsilon):\n",
        "\t\t\tnew_rank_vector = np.zeros(self.node_num)\n",
        "\t\t\tfor parent in self.edges:\n",
        "\t\t\t\tfor child in self.edges[parent]:\n",
        "\t\t\t\t\tnew_rank_vector[child] += (self.weight_vector[(parent,child)] / len(self.edges[parent]))\n",
        "\n",
        "\t\t\tleaked_rank = (1-sum(new_rank_vector))/self.node_num\n",
        "\t\t\tfinal_rank_vector = new_rank_vector + leaked_rank\n",
        "\t\t\tdiff = sum(abs(final_rank_vector - initial_rank_vector))\n",
        "\t\t\tinitial_rank_vector = final_rank_vector\n",
        "\t\t\titerations += 1\n",
        "\t\treturn final_rank_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE1kQkeWQ2ii"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "features_df['From'] = features_df['From_list']\n",
        "features_df['To'] = features_df['To_list']\n",
        "\n",
        "temp_df = features_df.groupby(['From','To'])['To'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(50)# Cat_1_level_2.unique()\n",
        "# temp_df = temp_df.map(lambda line: line.From != line.To)\n",
        "comparison_column = np.where(temp_df['From'] != temp_df['To'], True, False)\n",
        "temp_df['equal'] = comparison_column\n",
        "temp_df = temp_df.loc[temp_df['equal'] == True].head(20)\n",
        "from_users = temp_df['From'].to_list()\n",
        "to_users = temp_df['To'].to_list()\n",
        "node_num_dict = {}\n",
        "\n",
        "temp_list = to_users+from_users\n",
        "count = 1\n",
        "user_list = []\n",
        "\n",
        "#Create User Dictionary \n",
        "for user in temp_list:\n",
        "  if user not in node_num_dict:\n",
        "    node_num_dict[user] = count \n",
        "    user_list.append(user)\n",
        "    count += 1\n",
        "\n",
        "weight_dict = {}\n",
        "edge_list = []\n",
        "edges = defaultdict(list)\n",
        "weight_list = []\n",
        "\n",
        "#Create weighted dictionary \n",
        "for i in range(len(user_list)):\n",
        "  for j in range(i+1,len(user_list)):\n",
        "    if i == j:\n",
        "      continue\n",
        "    else:\n",
        "      edge_df = features_df[(features_df['From']==user_list[i]) & (features_df['To']==user_list[j])]\n",
        "      temp_count = edge_df['From'].count()\n",
        "      if temp_count != 0 :\n",
        "        weight_dict[(node_num_dict[user_list[i]], node_num_dict[user_list[j]])] = temp_count\n",
        "        weight_list.append(temp_count)\n",
        "        edge_list.append((node_num_dict[user_list[i]], node_num_dict[user_list[j]]))\n",
        "        edges[node_num_dict[user_list[i]]].append(node_num_dict[user_list[j]])\n",
        "\n",
        "      reverse_edge_df = features_df[(features_df['From']==user_list[j]) & (features_df['To']==user_list[i])]\n",
        "      temp_reverse_count = reverse_edge_df['From'].count()\n",
        "      if temp_reverse_count != 0 :\n",
        "        weight_dict[(node_num_dict[user_list[j]], node_num_dict[user_list[i]])] = temp_reverse_count\n",
        "        weight_list.append(temp_reverse_count)\n",
        "        edge_list.append((node_num_dict[user_list[j]], node_num_dict[user_list[i]]))\n",
        "        edges[node_num_dict[user_list[j]]].append(node_num_dict[user_list[i]])\n",
        "\n",
        "#Normalize weighted dictionary\n",
        "def NormalizeDict(wd,weight_list):\n",
        "    max_value = max(weight_list)\n",
        "    min_value = min(weight_list)\n",
        "    for node in weight_dict:\n",
        "      wd[node] = (wd[node]-min_value)/(max_value-min_value)\n",
        "    return wd\n",
        "\n",
        "weight_dict = NormalizeDict(weight_dict,weight_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg1CwdNbciav"
      },
      "source": [
        "Clean data and weight ranking framework\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsvtevliFQ4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be44e4a6-ff3e-400c-fa18-da78bca97355"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_data(df):\n",
        "  preprocessed_Subject, preprocessed_content = [], []\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "      stem_sub, stem_cont = [], []\n",
        "\n",
        "      if(not pd.isnull(row['subject_list'])):\n",
        "        # Removing punctuations from the sentence\n",
        "        sentence = re.sub(r'[^0-9A-Za-z]+', ' ', row['subject_list'])\n",
        "        for word in word_tokenize(sentence):\n",
        "            # Replace the word with stem word \n",
        "            stem_word = PorterStemmer().stem(word)\n",
        "            stem_cont.append(stem_word)\n",
        "      else:\n",
        "        stem_cont.append(' ')\n",
        "\n",
        "      if(not pd.isnull(row['Title'])):\n",
        "        # Removing punctuations from the sentence\n",
        "        sentence = re.sub(r'[^0-9A-Za-z]+', ' ', row['Title'])\n",
        "        for word in word_tokenize(sentence):\n",
        "            # Replace the word with stem word \n",
        "            stem_word = PorterStemmer().stem(word)\n",
        "            stem_sub.append(stem_word)\n",
        "        else:\n",
        "          stem_sub.append(' ')\n",
        "\n",
        "      preprocessed_Subject.append(' '.join([word for word in stem_sub]))\n",
        "      preprocessed_content.append(' '.join([word for word in stem_cont]))\n",
        "\n",
        "\n",
        "  df['processed_subject'] = preprocessed_Subject\n",
        "  df['processed_content'] = preprocessed_content\n",
        "  return df\n",
        "\n",
        "#Increase/Decrease weights based on response time\n",
        "def check_corpus(df):\n",
        "  urgency_list = ['soon', 'shortly','urgent', 'pressing','speedy', \n",
        "                  'prompt', 'straightaway', 'fast','promptly', 'quick', 'immediate', 'quickly', 'agile', 'ready']\n",
        "  total_count = 0\n",
        "  for word in urgency_list:\n",
        "    temp_df = df['processed_subject'][df['processed_subject'].str.contains(word, na=False)]\n",
        "    total_count += temp_df.count()\n",
        "  return total_count\n",
        "\n",
        "#Increase/Decrease weights based on response time\n",
        "def check_response_time(df):\n",
        "  df['Date'] =pd.to_datetime(df.Date_list)\n",
        "  df_sorted = df.sort_values(by='Date')\n",
        "  sorted_date_series = df_sorted['Date']\n",
        "  time_diff_series = sorted_date_series.diff()\n",
        "  in_digits = time_diff_series / np.timedelta64(1, 'D')\n",
        "  min_difference = in_digits.min()\n",
        "  if min_difference == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return (1.0/min_difference)\n",
        "\n",
        "#Compute Node Weight and exits to add more\n",
        "def check_node_weight(weight_dict, df, reverse_node_num):\n",
        "  for edge in weight_dict:\n",
        "    idx = np.where((df['From']==reverse_node_num[edge[0]]) & (df['To']==reverse_node_num[edge[1]]))\n",
        "    edge_mail_list = df.loc[idx]\n",
        "    cleaned_edge_mail_list = clean_data(edge_mail_list)\n",
        "    weight_dict[edge] += check_corpus(cleaned_edge_mail_list)\n",
        "    weight_dict[edge] += check_response_time(cleaned_edge_mail_list)\n",
        "    break\n",
        "    \n",
        "\n",
        "reverse_node_num = {}\n",
        "for node in node_num_dict:\n",
        "  reverse_node_num[node_num_dict[node]] = node\n",
        "\n",
        "check_node_weight(weight_dict,features_df, reverse_node_num)\n",
        "weight_list = []\n",
        "for weight in weight_dict:\n",
        "  weight_list.append(weight_dict[weight])\n",
        "weight_dict = NormalizeDict(weight_dict,weight_list)\n",
        "for weight in weight_dict:\n",
        "  # print(weight_dict[weight])\n",
        "  if np.isnan(weight_dict[weight]):\n",
        "    weight_dict[weight] = 0\n",
        "print(\"Node Weight Dictionary: \",weight_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Node Weight Dictionary:  {(1, 14): 1.0, (14, 1): 4.818800196641807e-07, (2, 14): 1.2926736742109169e-08, (14, 2): 3.159868981404463e-07, (3, 12): 0.0, (12, 3): 7.181520412282871e-10, (3, 14): 3.447129797895778e-08, (14, 3): 2.4417169401761765e-07, (18, 3): 0.0, (7, 4): 1.4363040824565743e-09, (4, 14): 1.3644888783337456e-08, (14, 4): 2.0467333175006182e-07, (4, 19): 0.0, (19, 4): 0.0, (5, 14): 4.596173063861038e-08, (14, 5): 1.7451094601847377e-07, (5, 18): 7.181520412282871e-10, (18, 5): 2.8726081649131486e-09, (8, 6): 0.0, (6, 14): 5.027064288598011e-09, (14, 6): 1.4219410416320084e-07, (7, 14): 4.021651430878409e-08, (14, 7): 1.1993139088512394e-07, (8, 10): 1.4363040824565743e-09, (8, 14): 9.335976535967731e-09, (14, 8): 1.0269574189564505e-07, (14, 9): 1.005412857719602e-07, (14, 10): 9.264161331844904e-08, (11, 14): 1.3644888783337456e-08, (14, 11): 8.833270107107931e-08, (12, 14): 2.0108257154392043e-08, (14, 12): 8.474194086493789e-08, (13, 14): 1.1490432659652594e-08, (14, 13): 7.612411637019843e-08, (14, 15): 5.386140309212154e-08, (15, 14): 3.5907602061414357e-09, (14, 16): 4.8116186762295234e-08, (16, 14): 1.4363040824565743e-09, (14, 17): 4.8116186762295234e-08, (17, 14): 3.5907602061414357e-09, (14, 18): 4.3807274514925514e-08, (18, 14): 1.0772280618424307e-08, (14, 19): 4.237097043246894e-08, (19, 14): 1.4363040824565743e-09}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lItfZwKDd6bv"
      },
      "source": [
        "Find synonyms and antonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh9RrbCsgr1f"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "def get_synonyms(word):\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for l in syn.lemmas():\n",
        "      synonyms.append(l.name())\n",
        "      if l.antonyms():\n",
        "          antonyms.append(l.antonyms()[0].name())\n",
        "  return (synonyms,antonyms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bVUBlUGd04L"
      },
      "source": [
        "Execute Page rank and display ranks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhsDkDnlkcoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c1d4d7-d6cf-4398-bf08-db89c05302d7"
      },
      "source": [
        "pr = PageRank(0.85, edges, 1e-6, 2, 22, weight_dict)\n",
        "PageRank_vector = pr.pageRank()\n",
        "page_rank_list = list(PageRank_vector)\n",
        "print(\"Rank for the stop 20 user interactions are: \")\n",
        "for i in range(1,20):\n",
        "  print(reverse_node_num[i],':',page_rank_list[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rank for the stop 20 user interactions are: \n",
            "suzanne.adams@enron.com : 1.4219773134477596e-08\n",
            "nmann@erac.com : 5.003488605381242e-09\n",
            "kathleen.carnahan@enron.com : 1.3728310636160166e-09\n",
            "carlos.sole@enron.com : -4.624463750784985e-10\n",
            "ben.jacoby@enron.com : -1.8987504575350728e-09\n",
            "sheila.tweed@enron.com : -4.651666615576839e-09\n",
            "ccampbell@kslaw.com : -5.888484019914445e-09\n",
            "pthompson@akllp.com : -6.846020074885495e-09\n",
            "reagan.rorschach@enron.com : -6.965712081756876e-09\n",
            "roseann.engeldorf@enron.com : -6.925814746133082e-09\n",
            "jkeffer@kslaw.com : -7.64396678736137e-09\n",
            "gregg.penman@enron.com : -7.843453465480337e-09\n",
            "heather.kroll@enron.com : -8.322221492965862e-09\n",
            "kay.mann@enron.com : 1.0000001237778564\n",
            "nwodka@bracepatt.com : -9.559038897303468e-09\n",
            "kathleen.clark@enron.com : -9.878217582293818e-09\n",
            "kent.shoemaker@ae.ge.com : -9.878217582293818e-09\n",
            "fred.mitro@enron.com : -9.758525575422437e-09\n",
            "jeffrey.hodge@enron.com : -1.0197396267284167e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHcF-aTCdkbf"
      },
      "source": [
        "Plot Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi94-IoImo2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbf6d67-a2ad-46e4-96ac-cb3f1f53d24d"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from graphviz import Digraph\n",
        "\n",
        "def generate_graph(rank_matrix, edge_list, node_num, weighted_vector, node_num_dict):\n",
        "  dot = Digraph(comment='The Round Table')\n",
        "  for edge in edge_list:\n",
        "    dot.edge(str(reverse_node_num[edge[0]]),str(reverse_node_num[edge[1]]))\n",
        "  dot.render('/content/drive/Shared drives/NLP Project/round-table15.gv', view=True)\n",
        "\n",
        "reverse_node_num = {}\n",
        "for node in node_num_dict:\n",
        "  reverse_node_num[node_num_dict[node]] = node\n",
        "generate_graph(PageRank_vector, edge_list, 21, weight_dict, reverse_node_num)\n",
        "print(\"Check /content/drive/Shared drives/NLP Project/ folder for the graph\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check /content/drive/Shared drives/NLP Project/ folder for the graph\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}