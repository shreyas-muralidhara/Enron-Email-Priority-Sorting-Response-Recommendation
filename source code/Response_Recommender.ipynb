{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Response Recommender.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyas-muralidhara/Enron-Email-Priority-Sorting-Response-Recommendation/blob/main/Response_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAe3Xv3tPcd3"
      },
      "source": [
        "## Response Recommender System for Enron Email Dataset\n",
        "\n",
        "CSC791 - Natural Language Processing Spring 2020\n",
        "\n",
        "Author - Shreyas Chikkballapur Muralidhara - schikkb  \n",
        "          Sharath Narayana - snaraya9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT85aH2v4vep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e81a2a-11be-4ca2-8840-d361e1320aac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRC2A8cgVyO"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJzgy9m2mg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3232e1-4a26-4786-d606-cc58a356a891"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xlrd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score,accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC,LinearSVC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU90bCnMLD1c"
      },
      "source": [
        "####Load the dataset for MANN-K profile to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3vU6Vm6COZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db07cedc-5d94-44a0-8fd3-0f81f1892c8b"
      },
      "source": [
        "email_df = pd.read_csv('/content/drive/Shared drives/NLP Project/test.csv',index_col=None,header=0)\n",
        "email_df.Date_list = email_df.Date_list.str.replace(',','')\n",
        "print(email_df.shape)\n",
        "print(\"The Label Distribution for the Email dataset:\\n\", email_df['Label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9571, 7)\n",
            "The Label Distribution for the Email dataset:\n",
            " thread    4957\n",
            "reply     4435\n",
            "delete     179\n",
            "Name: Label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6b236kVHF1n"
      },
      "source": [
        "### Preprocessing\n",
        "  * Removing punctutations  \n",
        "  *  Stemming using porter stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK5ThHWBHEjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b32648-50f1-4dd1-b57c-2a217fe3a6e2"
      },
      "source": [
        "# Perform text Preprocessing - Stemming using Porter Stemmer\n",
        "preprocessed_Subject, preprocessed_content = [], []\n",
        "\n",
        "for index, row in email_df.iterrows():\n",
        "    stem_sub, stem_cont = [], []\n",
        "\n",
        "    if(not pd.isnull(row['subject_list'])):\n",
        "      # Removing punctuations from the sentence\n",
        "      sentence = re.sub(r'[^0-9A-Za-z]+', ' ', row['subject_list'])\n",
        "      for word in word_tokenize(sentence):\n",
        "          # Replace the word with stem word \n",
        "          stem_word = PorterStemmer().stem(word)\n",
        "          stem_cont.append(stem_word)\n",
        "    else:\n",
        "      stem_cont.append(' ')\n",
        "\n",
        "    if(not pd.isnull(row['Title'])):\n",
        "      # Removing punctuations from the sentence\n",
        "      sentence = re.sub(r'[^0-9A-Za-z]+', ' ', row['Title'])\n",
        "      for word in word_tokenize(sentence):\n",
        "          # Replace the word with stem word \n",
        "          stem_word = PorterStemmer().stem(word)\n",
        "          stem_sub.append(stem_word)\n",
        "      else:\n",
        "        stem_sub.append(' ')\n",
        "\n",
        "    preprocessed_Subject.append(' '.join([word for word in stem_sub]))\n",
        "    preprocessed_content.append(' '.join([word for word in stem_cont]))\n",
        "\n",
        "\n",
        "email_df['processed_subject'] = preprocessed_Subject\n",
        "email_df['processed_content'] = preprocessed_content\n",
        "print(email_df.shape)\n",
        "print(email_df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9571, 9)\n",
            "Index(['To_list', 'From_list', 'Date_list', 'subject_list', 'Title', 'Label',\n",
            "       'file', 'processed_subject', 'processed_content'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF7C51vVBEP6"
      },
      "source": [
        "#### POS tagging\n",
        "  * Generate POS tags for the **email Subject(preprocessed)** and generate sentence with tags in \"word/POStag\" format.\n",
        "  * Generate POS tags for the **email Content(preprocessed)**  and generate sentence with tags in \"word/POStag\" format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANh5GjV4BE1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89540a62-b4b6-4849-a747-675b7168268d"
      },
      "source": [
        "# Method to compute the POS tags for an input column\n",
        "\n",
        "def generatePOS(sentence_list):\n",
        "  sentlist_posTagged = []\n",
        "\n",
        "  for sentence in sentence_list:\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    # Get the POS tags for the word tokens\n",
        "    POS_word_tokens = nltk.pos_tag(word_tokens)\n",
        "    \n",
        "    # Concatenate word/pos_tag format\n",
        "    sent_posTagged = ' '.join([entity[0]+\"/\"+entity[1] for entity in POS_word_tokens])    \n",
        "    sentlist_posTagged.append(sent_posTagged)\n",
        "  \n",
        "  return pd.DataFrame(sentlist_posTagged)\n",
        "\n",
        "  # Store the POS tagged Subject and Content into the dataframe\n",
        "\n",
        "email_df['SubjectPOS'] = generatePOS(email_df['processed_subject'])\n",
        "email_df['ContentPOS'] = generatePOS(email_df['processed_content'])\n",
        "\n",
        "print(email_df.shape)\n",
        "print(email_df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9571, 11)\n",
            "Index(['To_list', 'From_list', 'Date_list', 'subject_list', 'Title', 'Label',\n",
            "       'file', 'processed_subject', 'processed_content', 'SubjectPOS',\n",
            "       'ContentPOS'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_-a6orPGX0"
      },
      "source": [
        "#### Model 1 - Weighted Doc2Vec Sentence embedding\n",
        "  1. Generate sentence vectors for **POS tagged Subject** using Doc2Vec - Weight vectors 200\n",
        "  2. Generate sentence vectors for **POS tagged Content** using Doc2vec - Weight vectors 200\n",
        "  3. Generate sentence vectors for **From** using Doc2vec - Weight vectors 50\n",
        "  4. Generate sentence vectors for **To** using Doc2vec - Weight vectors 50\n",
        "  5. Generate sentence vectors for **Date** using Doc2vec - Weight vectors 50\n",
        "  6. Concatenate the vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY_fxZ6lPGFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f9442d-65e2-49ca-96d8-c49a766efe39"
      },
      "source": [
        "# Method to compute the Sentence embeddings for email fields From, To, Date, SubjectPOS and ContentPOS fields and combine them by concateation  \n",
        "def Doc2Vec_Emailvectors(To_list,From_list, email_date, Subject_list, Content_list):\n",
        "    # Step 1 - Generating tagged documents with list of words and their associated tags\n",
        "    df_from_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(From_list)]\n",
        "    df_to_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(To_list)]\n",
        "    df_date_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(email_date)]\n",
        "    df_subject_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(Subject_list)]\n",
        "    df_content_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(Content_list)]\n",
        "\n",
        "    # Step 2 - Define the model and build the vocab for the SubjectPOS tags\n",
        "    d2vmodel = Doc2Vec(min_count =1,vector_size=200, epochs=50)\n",
        "    d2vmodel.build_vocab(df_subject_tagged)\n",
        "\n",
        "    df_subject_vectors = []\n",
        "    for d in df_subject_tagged:  \n",
        "        df_subject_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "    df_subject_vectors = pd.DataFrame(df_subject_vectors)\n",
        "\n",
        "    # Step 3 - Define the model and build the vocab for the contentPOS tags\n",
        "    d2vmodel = Doc2Vec(min_count =1,vector_size=200, epochs=50)\n",
        "    d2vmodel.build_vocab(df_content_tagged)\n",
        "    df_content_vectors = []\n",
        "    for d in df_content_tagged:  \n",
        "        df_content_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "    df_content_vectors = pd.DataFrame(df_content_vectors)\n",
        "\n",
        "    # Concatenate the 2 feature vectors into single feature vector\n",
        "    df_subjectContent_vectors = np.concatenate((df_subject_vectors, df_content_vectors), axis=1)\n",
        "\n",
        "    # Step 3 - Define the model and build the vocab for the From tags\n",
        "    d2vmodel = Doc2Vec(min_count =1,vector_size=50, epochs=50)\n",
        "    d2vmodel.build_vocab(df_from_tagged)\n",
        "    df_from_vectors = []\n",
        "    for d in df_from_tagged:  \n",
        "        df_from_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "    df_from_vectors = pd.DataFrame(df_from_vectors)\n",
        "\n",
        "    # Step 4 - Define the model and build the vocab for the To tags\n",
        "    d2vmodel = Doc2Vec(min_count =1,vector_size=50, epochs=50)\n",
        "    d2vmodel.build_vocab(df_to_tagged)\n",
        "    df_to_vectors = []\n",
        "    for d in df_to_tagged:  \n",
        "        df_to_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "    df_to_vectors = pd.DataFrame(df_to_vectors)\n",
        "\n",
        "    # Concatenate the 2 feature vectors into single feature vector\n",
        "    df_fromTo_vectors = np.concatenate((df_from_vectors, df_to_vectors), axis=1)\n",
        "\n",
        "    df_fromToContent_vectors = np.concatenate((df_subjectContent_vectors,df_fromTo_vectors), axis=1)\n",
        "\n",
        "     # Step 5 - Define the model and build the vocab for the Date tags\n",
        "    d2vmodel = Doc2Vec(min_count =1,vector_size=50, epochs=50)\n",
        "    d2vmodel.build_vocab(df_date_tagged)\n",
        "    df_date_vectors = []\n",
        "    for d in df_date_tagged:  \n",
        "        df_date_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "    df_date_vectors = pd.DataFrame(df_date_vectors)\n",
        "\n",
        "    df_EmailContent_vectors = np.concatenate((df_fromToContent_vectors,df_date_vectors), axis=1)\n",
        "    print(df_EmailContent_vectors.shape)\n",
        "    \n",
        "    return pd.DataFrame(df_EmailContent_vectors)\n",
        "\n",
        "# Generate Concatenated vectors for Train dataset  \n",
        "df_train_emailCombined_vectors = Doc2Vec_Emailvectors(email_df['To_list'], email_df['From_list'], email_df['Date_list'], email_df['SubjectPOS'], email_df['ContentPOS'])\n",
        "\n",
        "#Split the input data into Training and validation sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df_train_emailCombined_vectors, email_df['Label'], test_size=0.25, shuffle = True, stratify = email_df['Label'], random_state=0)\n",
        "print(X_train.shape, X_test.shape,Y_train.shape, Y_test.shape)\n",
        "\n",
        "print(\"The Label Distribution for the Training dataset:\\n\", Y_train.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9571, 550)\n",
            "(7178, 550) (2393, 550) (7178,) (2393,)\n",
            "The Label Distribution for the Training dataset:\n",
            " thread    3718\n",
            "reply     3326\n",
            "delete     134\n",
            "Name: Label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZmYGpE3Roxo"
      },
      "source": [
        "### Model 1 - Train and Test the Doc2Vec generated vectors using SVM\n",
        "  Ground truth measured using   \n",
        "  * Accuracy  \n",
        "  * F1 Score  \n",
        "  * Classification Metrics  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7A914K8ttTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee7dad0-ddd7-4b1a-9fd1-346d1176b776"
      },
      "source": [
        "# Fit the model for the classifier\n",
        "clf_doc2vec = SVC(max_iter =10000, C=1).fit(X_train, Y_train)\n",
        "\n",
        "# Predicting the class labels for Test data\n",
        "Y_test_pred = clf_doc2vec.predict(X_test)\n",
        "\n",
        "print('Classification Model 1 - SVM - Doc2Vec TEST metrics:\\nAccuracy -',round(accuracy_score(Y_test,Y_test_pred),4))\n",
        "print('f1 score -', round(f1_score(Y_test,Y_test_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(Y_test, Y_test_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Model 1 - SVM - Doc2Vec TEST metrics:\n",
            "Accuracy - 0.6849\n",
            "f1 score - 0.6623\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      delete       1.00      0.04      0.09        45\n",
            "       reply       0.61      0.96      0.74      1109\n",
            "      thread       0.89      0.46      0.61      1239\n",
            "\n",
            "    accuracy                           0.68      2393\n",
            "   macro avg       0.83      0.49      0.48      2393\n",
            "weighted avg       0.76      0.68      0.66      2393\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1I14ssf5ZnC"
      },
      "source": [
        "### Model 2 - Tf-Idf (Term frequency - Inverse Document frequency)\n",
        "\n",
        "#### Step 1 - Convert the collection of email corpus to matrix of token counts using Count Vectorizer\n",
        "\n",
        "#### Step 2 - Transform the count matrix into normalized TF or TF-IDF form.\n",
        "\n",
        "#### Step 3 - Train and test using the classifier for each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PDeOFwZ-N9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1484e45-a50e-4842-8e95-d47643decb93"
      },
      "source": [
        "# Split the sentences into training and validation by stratifying the samples.\n",
        "email_df['SubjectPOS'] = [''.join([word for word in subject]) for subject in email_df['SubjectPOS']]\n",
        "email_df['ContentPOS'] = [''.join([word for word in content]) for content in email_df['ContentPOS']]\n",
        "\n",
        "email_df['Merged_email'] = email_df[['To_list','From_list','Date_list','SubjectPOS','ContentPOS']].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(email_df['Merged_email'], email_df['Label'], test_size=0.25, shuffle = True, stratify = email_df['Label'], random_state=0)\n",
        "#print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "print(\"The Label Distribution for the Training dataset:\\n\", Y_train.value_counts())\n",
        "\n",
        "# Step 1 - Generate the token matrix\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "X_test_counts = count_vect.transform(X_test)\n",
        "\n",
        "# Step 2 - Transform the count matrix to TF-IDF\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "#print(X_train_tfidf.shape, X_test_tfidf.shape,Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Label Distribution for the Training dataset:\n",
            " thread    3718\n",
            "reply     3326\n",
            "delete     134\n",
            "Name: Label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_gYkr4lLy7t"
      },
      "source": [
        "### Model 2 - Train and Test the TfIdf generated frequency corpus using Naive \n",
        "  Ground truth measured using   \n",
        "  * Accuracy  \n",
        "  * F1 Score  \n",
        "  * Classification Metrics  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqsEYDelMMU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdf02c4-55ac-4215-9bf7-3e91030636e2"
      },
      "source": [
        "# Fit the model for the classifier\n",
        "clf = MultinomialNB().fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# Predicting the class labels for Test data\n",
        "Y_test_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "print('Classification Model 2 - K- Neighbors classifier Classifier Tf-Idf TEST metrics:\\nAccuracy -',round(accuracy_score(Y_test,Y_test_pred),4))\n",
        "print('f1 score -', round(f1_score(Y_test,Y_test_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(Y_test, Y_test_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Model 2 - K- Neighbors classifier Classifier Tf-Idf TEST metrics:\n",
            "Accuracy - 0.5357\n",
            "f1 score - 0.5148\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      delete       0.00      0.00      0.00        45\n",
            "       reply       0.52      0.35      0.42      1109\n",
            "      thread       0.54      0.72      0.62      1239\n",
            "\n",
            "    accuracy                           0.54      2393\n",
            "   macro avg       0.35      0.36      0.35      2393\n",
            "weighted avg       0.52      0.54      0.51      2393\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}